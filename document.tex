\documentclass[13pt]{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{dsfont}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{plain}
\newtheorem{defi}[thm]{Definition}
\newtheorem*{ex}{Example}

\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage[backend = biber, style=numeric-comp,
citestyle=numeric, sorting=nyt]{biblatex}
\addbibresource{literature.bib}

\usepackage{xcolor}
\usepackage[hidelinks,urlcolor=blue]{hyperref}
\hypersetup{
    colorlinks,
    linkcolor=black,
    citecolor=black,
    urlcolor={blue!80!black},
}

\usepackage[left=3cm, top=1.5cm, includeheadfoot]{geometry}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\NN}{\mathcal{N}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\E}{\bm{\mathrm{E}}}
\renewcommand{\P}{\bm{\mathrm{P}}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\set}[1]{\{#1\}}


\begin{document}

\tableofcontents

\newpage

\section{Introduction}

\subsection{Why singular values?}

Text: Vershynin \cite{non_asym_random_matrices}

\begin{itemize}
    \item Linear equations $y = Ax$ are the simplest possible approximation
        for any continuous model
    \item Taylor's theorem: For small variable range, we have a good
        approximation (under some regularity conditions)
\end{itemize}

\subsection{Why random matrices?}

\begin{enumerate}
    \item Compressed Sensing
        \begin{itemize}
            \item For square matrix case: identity is perfectly well-conditioned
            \item For a flat rectangular matrix $A \in \R^{m \times N}, m < N$, can
                we choose $A$ such that all $k$-column submatrices are approximate isometries?
        \end{itemize}
    \item Dimension reduction
        \begin{itemize}
            \item $p$ points $\{x_1,\ldots,x_p\}$ in $\R^N$
            \item can we project $\R^N \to \R^m$ using a matrix $A$ such that
                the geometry is preserved?
                \[
                    (1-\varepsilon)\|x_i-x_j\| \le \|Ax_i - Ax_j\| \le (1+\varepsilon)\|x_i - x_j\|
                \]
            \item deterministic methods \underline{must} adapt to $\{x_1,\ldots,x_p\}$; no single
                matrix will work for all sets
        \end{itemize}
\end{enumerate}

\subsection{Asymptotic and non-asymptotic regimes}

Observation: $N, n \to \infty \implies $ spectrum of $A$ stabilizes \\
Mathematical formulation: Limit laws (random matrix version of Central Limit Theorem)

\begin{ex}[Bai-Yin Law]
    $A \in \R^{n \times n}$, entries independent standard normal random variables.
    Then
    \[
        \frac{s_{\max}(A)}{2\sqrt{n}} \to 1 \quad \text{ a.s. as } n \to \infty.
    \]
\end{ex}

Not enough for finite dimensions (no information about the rate). In every dimension one has
\[
    s_{\max}(A) \le C\sqrt{n},
\]
with probability at least $1 - C'\e^{-n}$.

\subsection{Guiding paradigm}

Tall random matrices should act as approximate isometries.
A $N \times n$ random matrix $A$ with $\gg$ should satisfy
\[
    \begin{aligned}
        (1-\delta)k\|x\|_2 \le \|Ax\|_2 \le (1+\delta)k\|x\|_2
    \end{aligned}
\]
with high probability.

\subsection{In this class}

We study (tall) random matrices with independent rows or independent columns
and either
\begin{itemize}
    \item strong moment assumptions (subgaussian)
    \item no moment assumptions, except finite variance (heavy-tailed)
\end{itemize}

\section{Preliminaries}

\subsection{Matrices and their singular values}

We mostly study tall $A \in \R^{N \times n}$.

\begin{defi}
    The numbers $s_1(A) \ge s_2(A) \ge \ldots \ge s_1(A) \ge 0$, such that
    $s_i^2(A)$ are the eigenvalues of $A^\ast A$, are called the \emph{singular values} of $A$.
    We also write
    \[
        \begin{aligned}
            s_{\max}(A) := s_1(A), \quad s_{\min}(A) := s_n(A) .
        \end{aligned}
    \]
\end{defi}

\begin{defi}
    We define the \emph{spectral norm} as
    \[
        \begin{aligned}
            \|A\| = \|A\|_{\ell^2 \to \ell^2} = \sup_{x \in \S^{n-1}} \|Ax\|_2.
        \end{aligned}
    \]
\end{defi}

$s_{\max}(A) = \|A\|$ and $s_{\min}(A) = \|A^\dagger\|^{-1}$, where $A^\dagger$ is the pseudoinverse.

\begin{defi}
    Let $A \in \R^{N \times n}$ or $\C^{N \times n}$ with singular values $s := (s_1,\ldots,s_n)$.
    Let $1 \le p \le \infty$. Then the \emph{Schatten-$p$-norm} is defined as
    \[
        \begin{aligned}
            \|A\|_{S^p} := \|s\|_p.
        \end{aligned}
    \]
    The Schatten-$2$-norm is also called \emph{Frobenius norm} and denoted by $\|\cdot\|_F$.
\end{defi}

\subsection{Nets}

\begin{defi}
    Let $(X,d)$ be a metric space and let $\varepsilon > 0$.
    A subset $\NN_\varepsilon$ of $X$ is called an \emph{$\varepsilon$-net} of $X$ if
    every point $x \in X$ can be approximated to an accuracy of $\varepsilon$ by
    some point $y \in \NN_\varepsilon$, i.e., s.t. $d(x,y) \le \varepsilon$.
    The minimal cardinality of an $\varepsilon$-net of $X$, if finite, is
    denoted $\NN(X,\varepsilon)$ and is called the \emph{covering number} of $X$.
\end{defi}

$X$ is compact $\iff \NN(X,\varepsilon)$ is finite.

\begin{lem}[Covering number of the sphere]
    Consider the vector space $\R^n$ equipped with the norm
    $\||\cdot\||$ and $S = \set{x \in \R^n : \||x\|| = 1}$ be the associated
    unit sphere. Then for every $\varepsilon > 0$ one has
    \[
        \begin{aligned}
            \NN(S, \varepsilon) \le (1 + \frac{2}{\varepsilon})^n.
        \end{aligned}
    \]
\end{lem}

\begin{lem}
    Let $A$ be an $N \times n$ matrix and let $\NN_\varepsilon$ be an
    $\varepsilon$-net of $S^{n-1}$ w.r.t. the $\ell_2$-norm for some
    $\varepsilon \in [0,1)$. Then
    \[
        \begin{aligned}
            \max_{x \in \NN_\varepsilon} \|A\|_2 \le \|A\|_2 \le (1-\varepsilon)^{-1} \max_{x \in \NN_\varepsilon} \|A\|_2.
        \end{aligned}
    \]
\end{lem}

\begin{lem}
    Let $A$ be a symmetric $n \times n$ matrix and let $\NN_\varepsilon$ be
    an $\varepsilon$-net of $S^{n-1}$ w.r.t. $\|\cdot\|_2$ for some $\varepsilon \in [0,1)$.
    Then
    \[
        \|A\| =
        \sup_{x \in S^{n-1}} |\langle Ax, x\rangle|
        \le (1-2\varepsilon)^{-1} \max_{x \in \NN_\varepsilon} | \langle Ax, x \rangle| 
    \]
\end{lem}

\subsection{Non-asymptotic results in one dimension}

Most result discussed in probability classes are asymptotic.
Here are some non-asymptotic variants

\begin{prop}[Hoeffding's inequality]
Let $X_1, \ldots, X_n$ be a sequence of independent, real-valued random
variables such that $\E(X_\ell) = 0$ and $|X_\ell| \le B_\ell$ a.s. for all
$\ell = 1, \ldots, n$ for some $B_\ell > 0$. Then
\[
    \begin{aligned}
        \P \left( \sum_{\ell=1}^{n} X_\ell > t \right) \le \exp \left( - \frac{t^2}{2 \sum_{\ell=1}^{n} B_\ell^2} \right).
    \end{aligned}
\]
\end{prop}

\begin{prop}[Bernstein type inequality]
    Let $X_1, \ldots, X_n$ be independent mean-zero random variables
    such that
    \[
        \begin{aligned}
            \E\left( |X_\ell|^m  \right) \le m! K^{m-2} \frac{\sigma_\ell}{2}
        \end{aligned}
    \] for all $\ell = 1, \ldots, n$ and all $m \in \N_{n \ge 2}$,
    for some constants $K > 0$ and $\sigma_\ell > 0$, $\ell = 1, \ldots, n$.
    Then for all $t > 0$ one has
    \[
        \begin{aligned}
            \P\left( \left|\sum_{\ell=1}^{n} X_\ell\right| > t \right)
            \le 2 \exp(-\frac{t^2}{\sigma^2 + Kt})
        \end{aligned}
    \]
    where $simga^2 := \sum_{\ell=1}^{n} \sigma_\ell^2$.
\end{prop}

\begin{lem}
    Let $\xi = (\xi_1, \ldots, \xi_M)$ be a sequence of independent random variables with $\E \xi_j = 0$ for all $j = 1, \ldots, M$. Let $A_{jk}$, $j, k = 1, \ldots, M$ be a doubly indexed sequence of elements in a vector space $X$. Let $F: X \to \R$ be a convex function.
    Then
    \[
        \begin{aligned}
            \E F\left( \sum_{\substack{j,k = 1\\ j \ne k}}^{M} \xi_j\xi_kA_{jk} \right) \le \E F\left( 4 \sum_{j=k=1}^{M} \xi_j\xi_k'A_{jk} \right).
        \end{aligned}
    \]
    Here $\xi_j'$ is an independent copy of $\xi_j$.
\end{lem} 

\begin{thm}[Tail estimates for Rademacher chaos]
    Let $A \in \R^{M \times M}$ be a symmetric matrix with zero
    diagonal and $\varepsilon$ a Rademacher vector.
    Consider the Rademacher chaos
    \[
        \begin{aligned}
            X = \sum_{j,k = 1}^{M} \varepsilon_j\varepsilon_k A_{jk}.
        \end{aligned}
    \]
    Then
    \[
        \begin{aligned}
            \P\left( |X| \ge t \right) \le \begin{cases}
                2 \exp\left( - \frac{3t^2}{128\|A\|_F^2} \right) & \text{ if } 0 < t \le \frac{4 \|A\|_F^2}{3\|A\|} \\
                2 \exp\left( - \frac{t}{32\|A\|} \right) & \text{ if } t > \frac{4\|A\|_F^2}{3\|A\|}
            \end{cases} 
            .
        \end{aligned}
    \]
\end{thm}

\subsection{Subgaussian random variables}

\begin{lem}[Equivalence of subgaussian properties]
    Let $X$ be a random variable. Then the following properties
    are equivalent with parameters $K_i > 0$ differing from each other by at most an absolute constant factor.
    More precisely, there exists an absolute constant $C$ s.t.
    property $i$ implies property $j$ with parameter $K_j \le CK_i$
    for any two properties $i,j = 1,2,3$.
    \begin{enumerate}
        \item Tails: $\P(|X|>t) \le \exp\left( 1 - \frac{t^2}{K_1^2} \right) $ for all $t \ge 0$.
        \item Moments: $\left( \E|X|^P \right)^{\frac{1}{p}} \le K_2 \sqrt{p}$ for all $p \ge 1$.
        \item Superexponential moment: $\E\exp(\frac{X^2}{K_3^2} \le \e$.
    \end{enumerate}
    Moreover, if $\E X = 0$, then properties 1-3 are also equivalent to the following one:
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item Moment generating function: $\E\exp(tX) \le \exp(t^2K_4^2)$ for all $t \in \R$.
    \end{enumerate}
\end{lem}

\newpage

\printbibliography[heading=bibintoc]

\end{document}
