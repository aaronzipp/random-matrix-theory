\documentclass[13pt]{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{dsfont}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{plain}
\newtheorem{defi}[thm]{Definition}
\newtheorem*{ex}{Example}

\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage[backend = biber, style=numeric-comp,
citestyle=numeric, sorting=nyt]{biblatex}
\addbibresource{literature.bib}

\usepackage{xcolor}
\usepackage[hidelinks,urlcolor=blue]{hyperref}
\hypersetup{
    colorlinks,
    linkcolor=black,
    citecolor=black,
    urlcolor={blue!80!black},
}

\usepackage[left=3cm, top=1.5cm, includeheadfoot]{geometry}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\NN}{\mathcal{N}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\V}{\mathrm{Var}}
\newcommand{\E}{\bm{\mathrm{E}}}
\newcommand{\e}{\mathrm{e}}


\begin{document}

\tableofcontents

\newpage

\section{Introduction}

\subsection{Why singular values?}

Text: Vershynin \cite{non_asym_random_matrices}

\begin{itemize}
    \item Linear equations $y = Ax$ are the simplest possible approximation
        for any continuous model
    \item Taylor's theorem: For small variable range, we have a good
        approximation (under some regularity conditions)
\end{itemize}

\subsection{Why random matrices?}

\begin{enumerate}
    \item Compressed Sensing
        \begin{itemize}
            \item For square matrix case: identity is perfectly well-conditioned
            \item For a flat rectangular matrix $A \in \R^{m \times N}, m < N$, can
                we choose $A$ such that all $k$-column submatrices are approximate isometries?
        \end{itemize}
    \item Dimension reduction
        \begin{itemize}
            \item $p$ points $\{x_1,\ldots,x_p\}$ in $\R^N$
            \item can we project $\R^N \to \R^m$ using a matrix $A$ such that
                the geometry is preserved?
                \[
                    (1-\varepsilon)\|x_i-x_j\| \le \|Ax_i - Ax_j\| \le (1+\varepsilon)\|x_i - x_j\|
                \]
            \item deterministic methods \underline{must} adapt to $\{x_1,\ldots,x_p\}$; no single
                matrix will work for all sets
        \end{itemize}
\end{enumerate}

\subsection{Asymptotic and non-asymptotic regimes}

Observation: $N, n \to \infty \implies $ spectrum of $A$ stabilizes \\
Mathematical formulation: Limit laws (random matrix version of Central Limit Theorem)

\begin{ex}[Bai-Yin Law]
    $A \in \R^{n \times n}$, entries independent standard normal random variables.
    Then
    \[
        \frac{s_{\max}(A)}{2\sqrt{n}} \to 1 \quad \text{ a.s. as } n \to \infty.
    \]
\end{ex}

Not enough for finite dimensions (no information about the rate). In every dimension one has
\[
    s_{\max}(A) \le C\sqrt{n},
\]
with probability at least $1 - C'\e^{-n}$.

\subsection{Guiding paradigm}

Tall random matrices should act as approximate isometries.
A $N \times n$ random matrix $A$ with $\gg$ should satisfy
\[
    \begin{aligned}
        (1-\delta)k\|x\|_2 \le \|Ax\|_2 \le (1+\delta)k\|x\|_2
    \end{aligned}
\]
with high probability.

\subsection{In this class}

We study (tall) random matrices with independent rows or independent columns
and either
\begin{itemize}
    \item strong moment assumptions (subgaussian)
    \item no moment assumptions, except finite variance (heavy-tailed)
\end{itemize}

\section{Preliminaries}

\subsection{Matrices and their singular values}

We mostly study tall $A \in \R^{N \times n}$.

\begin{defi}
    The numbers $s_1(A) \ge s_2(A) \ge \ldots \ge s_1(A) \ge 0$, such that
    $s_i^2(A)$ are the eigenvalues of $A^\ast A$, are called the \emph{singular values} of $A$.
    We also write
    \[
        \begin{aligned}
            s_{\max}(A) := s_1(A), \quad s_{\min}(A) := s_n(A) .
        \end{aligned}
    \] 
\end{defi} 

\begin{defi}
    We define the \emph{spectral norm} as
    \[
        \begin{aligned}
            \|A\| = \|A\|_{\ell^2 \to \ell^2} = \sup_{x \in \S^{n-1}} \|Ax\|_2.
        \end{aligned}
    \] 
\end{defi} 

$s_{\max}(A) = \|A\|$ and $s_{\min}(A) = \|A^\dagger\|^{-1}$, where $A^\dagger$ is the pseudoinverse.

\begin{defi}
    Let $A \in \R^{N \times n}$ or $\C^{N \times n}$ with singular values $s := (s_1,\ldots,s_n)$.
    Let $1 \le p \le \infty$. Then the \emph{Schatten-$p$-norm} is defined as
    \[
        \begin{aligned}
            \|A\|_{S^p} := \|s\|_p.
        \end{aligned}
    \]
    The Schatten-$2$-norm is also called \emph{Frobenius norm} and denoted by $\|\cdot\|_F$.
\end{defi}

\subsection{Nets}

\begin{defi}
    Let $(X,d)$ be a metric space and let $\varepsilon > 0$.
    A subset $\NN_\varepsilon$ of $X$ is called an \emph{$\varepsilon$-net} of $X$ if
    every point $x \in X$ can be approximated to an accuracy of $\varepsilon$ by
    some point $y \in \NN_\varepsilon$, i.e., s.t. $d(x,y) \le \varepsilon$.
    The minimal cardinality of an $\varepsilon$-net of $X$, if finite, is
    denoted $\NN(X,\varepsilon)$ and is called the \emph{covering number} of $X$.
\end{defi}

$X$ is compact $\iff \NN(X,\varepsilon)$ is finite.

\newpage

\printbibliography[heading=bibintoc]

\end{document}
